Practical No: 5

# Load the required library
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)

# Load the Iris dataset
data(iris)

# Split the dataset into training and testing sets
set.seed(123) # for reproducibility
train_index <- sample(1:nrow(iris), 0.7 * nrow(iris)) # 70% training, 30% testing
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Create and train the decision tree model
decision_tree_model <- rpart(Species ~ ., data = train_data, method = "class")

# Display the decision tree
rpart.plot(decision_tree_model, main="Decision Tree for Iris Dataset")

# Make predictions on the testing set
predictions <- predict(decision_tree_model, test_data, type = "class")

# Evaluate the model

accuracy <- mean(predictions == test_data$Species)
print(paste("Accuracy:", accuracy))

# Display confusion matrix
confusion_matrix <- table(predictions, test_data$Species)
print("Confusion Matrix:")
print(confusion_matrix)

Practical No: 6

# Load the required library
install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)

# Load the Iris dataset
data(iris)

# Split the dataset into training and testing sets
set.seed(123) # for reproducibility
train_index <- sample(1:nrow(iris), 0.7 * nrow(iris)) # 70% training, 30% testing
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Create and train the decision tree model
decision_tree_model <- rpart(Species ~ ., data = train_data, method = "class")

# Display the decision tree
rpart.plot(decision_tree_model, main="Decision Tree for Iris Dataset")

# Make predictions on the testing set
predictions <- predict(decision_tree_model, test_data, type = "class")

# Evaluate the model

accuracy <- mean(predictions == test_data$Species)
print(paste("Accuracy:", accuracy))

# Display confusion matrix
confusion_matrix <- table(predictions, test_data$Species)
print("Confusion Matrix:")
print(confusion_matrix)

Practical No: 7

# Load libraries
library(ggplot2)
library(dplyr)
library(caTools) # for splitting data

# Load data
data <- read.csv("D:/SYDS/admission.csv") # Make sure to replace "your_data_file.csv" with
your actual file name

# Explore data
head(data)
summary(data)

# Preprocess data if needed (e.g., handle missing values, scale features)

# Split data into training and testing sets
set.seed(123)
split <- sample.split(data$admit, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)

# Fit logistic regression model
model <- glm(admit ~ gre + gpa + rank, data = train_data, family = "binomial")

# Summary of the model
summary(model)

# Predictions on test data
predictions <- predict(model, newdata = test_data, type = "response")

# Convert probabilities to binary outcomes (0 or 1)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)

# Evaluate the model
accuracy <- mean(predicted_classes == test_data$admit)
cat("Accuracy on test set:", accuracy, "\n")

# Check if the model is a good fit (e.g., by examining residuals, ROC curve, etc.)

# Plot ROC curve
library(pROC)
roc_curve <- roc(test_data$admit, predictions)
plot(roc_curve, main = "ROC Curve", col = "blue")
auc <- auc(roc_curve)
legend("bottomright", legend = paste("AUC =", round(auc, 2)), col = "blue", lwd = 2)

# Check for the required library in R
if (!require(MASS)) {
  install.packages("MASS")
  library(MASS)
}

Practical No: 8

# Load necessary libraries
library(ggplot2)

# Load the dataset
data <- read.csv("D:/SYDS/admission.csv") # Replace "admission_dataset.csv" with your
actual file name
head(data)
# Explore the structure and summary statistics of the dataset
str(data)
summary(data)

# Fit multiple regression model
model <- lm(admit ~ gre + gpa + rank, data = data)

# Summary of the model
summary(model)

Practical No: 9

install.packages("class")
# Load required library
library(class)

# Load dataset (example using built-in Iris dataset)
data(iris)

# Split dataset into train and test sets
set.seed(123) # For reproducibility
trainIndex <- sample(1:nrow(iris), 0.8 * nrow(iris))
data_train <- iris[trainIndex, ]
data_test <- iris[-trainIndex, ]

# Define the number of nearest neighbors (k)
k <- 5

# Train KNN model
knn_model <- knn(train = data_train[, -5], test = data_test[, -5], cl = data_train$Species, k
= k)

# Evaluate performance
conf_matrix <- table(Actual = data_test$Species, Predicted = knn_model)

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- diag(conf_matrix) / colSums(conf_matrix)
recall <- diag(conf_matrix) / rowSums(conf_matrix)
f1_score <- 2 * precision * recall / (precision + recall)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(paste("Accuracy:", accuracy))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))

Practical No: 10

a. Clustering algorithms for unsupervised classification.
Source Code:

install.packages("stats")
# Load required library
library(stats)

# Generate sample data
set.seed(123)
data <- matrix(rnorm(200), ncol = 2)

# Perform K-means clustering
kmeans_result <- kmeans(data, centers = 3) # 3 clusters

# Print the cluster centers
print(kmeans_result$centers)

# Print the cluster membership of each point
print(kmeans_result$cluster)

b. Plot the cluster data using R visualizations.
Source Code:

install.packages("stats")
# Load required libraries
library(stats)

# Generate sample data
set.seed(123)
data <- matrix(rnorm(200), ncol = 2)

# Perform K-means clustering
kmeans_result <- kmeans(data, centers = 3) # 3 clusters

# Plot the clustered data
plot(data, col = kmeans_result$cluster, pch = 20, main = "K-means Clustering")
points(kmeans_result$centers, col = 1:3, pch = 8, cex = 2) # Plot cluster centers
legend("topright", legend = 1:3, col = 1:3, pch = 8, title = "Clusters")